{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e17632d",
   "metadata": {
    "id": "DwISRO5aTn3B",
    "papermill": {
     "duration": 0.004431,
     "end_time": "2025-12-02T05:26:30.544554",
     "exception": false,
     "start_time": "2025-12-02T05:26:30.540123",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"background-color: black; color: white; padding: 10px;text-align: center;\">\n",
    "  <strong>Date Published:</strong> December 2, 2025 <strong>Author:</strong> Adnan Alaref\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309248ac",
   "metadata": {
    "id": "B1-kcjJFTnqf",
    "papermill": {
     "duration": 0.00324,
     "end_time": "2025-12-02T05:26:30.551571",
     "exception": false,
     "start_time": "2025-12-02T05:26:30.548331",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# üß† Introduction to Vanilla Recurrent Neural Networks (RNNs)\n",
    "\n",
    "Recurrent Neural Networks (RNNs) are one of the simplest and most fundamental ways to model **sequential data** such as text, audio, and time-series.\n",
    "\n",
    "However, many beginners struggle to understand **what actually happens inside an RNN**, especially during **Backpropagation Through Time (BPTT)**.\n",
    "\n",
    "In this notebook, we will build a **vanilla RNN completely from scratch** using basic PyTorch tensor operations ‚Äî  \n",
    "**no `nn.RNN`, no autograd**, only pure math and matrix operations.\n",
    "\n",
    "This approach will help you build deep and intuitive understanding of how RNNs work internally.\n",
    "\n",
    "---\n",
    "\n",
    "## üîç What You Will Learn\n",
    "\n",
    "### ‚úîÔ∏è Forward Pass\n",
    "- How a single RNN step computes the next hidden state using:\n",
    "\n",
    "$$\n",
    "h_t = \\tanh(x_t W_x + h_{t-1} W_h + b)\n",
    "$$\n",
    "\n",
    "- How hidden states flow across timesteps in a sequence.\n",
    "\n",
    "### ‚úîÔ∏è Backward Pass (BPTT)\n",
    "- How gradients move backward through time.\n",
    "- How to compute:\n",
    "  - gradients w.r.t **input**\n",
    "  - gradients w.r.t **previous hidden state**\n",
    "  - gradients w.r.t **weights** and **biases**\n",
    "\n",
    "### ‚úîÔ∏è Concepts & Intuition\n",
    "- Why **vanishing** and **exploding gradients** happen.\n",
    "- How deep-learning frameworks compute RNN gradients under the hood.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Why This Notebook Is Useful\n",
    "\n",
    "- Builds **real understanding** ‚Äî not just API usage.\n",
    "- Helps you debug **training instabilities** in sequence models.\n",
    "- Prepares you for more advanced models:\n",
    "  - LSTM  \n",
    "  - GRU  \n",
    "  - Transformers  \n",
    "- Makes you a stronger ML engineer because you understand **the math behind the frameworks**.\n",
    "\n",
    "---\n",
    "\n",
    "Let's get started and open the RNN ‚Äúblack box‚Äù together üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02c7601",
   "metadata": {
    "papermill": {
     "duration": 0.00325,
     "end_time": "2025-12-02T05:26:30.557920",
     "exception": false,
     "start_time": "2025-12-02T05:26:30.554670",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <a id=\"Import\"></a><div style=\"background: linear-gradient(to right, #1b5e20, #2e7d32, #388e3c, #43a047, #4caf50); font-family: 'Times New Roman', serif; font-size: 28px; font-weight: bold; text-align: center; border-radius: 15px; padding: 15px; border: 2px solid #ffffff; box-shadow: 0 4px 10px rgba(0, 0, 0, 0.2); -webkit-background-clip: text; -webkit-text-fill-color: transparent;\">Step 1: Import Library.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac60cddd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T05:26:30.565879Z",
     "iopub.status.busy": "2025-12-02T05:26:30.565499Z",
     "iopub.status.idle": "2025-12-02T05:26:35.665302Z",
     "shell.execute_reply": "2025-12-02T05:26:35.664376Z"
    },
    "id": "tVVC_2jH1321",
    "papermill": {
     "duration": 5.105956,
     "end_time": "2025-12-02T05:26:35.667201",
     "exception": false,
     "start_time": "2025-12-02T05:26:30.561245",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b1b19a",
   "metadata": {
    "papermill": {
     "duration": 0.003502,
     "end_time": "2025-12-02T05:26:35.674289",
     "exception": false,
     "start_time": "2025-12-02T05:26:35.670787",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <a id=\"Import\"></a><div style=\"background: linear-gradient(to right, #1b5e20, #2e7d32, #388e3c, #43a047, #4caf50); font-family: 'Times New Roman', serif; font-size: 28px; font-weight: bold; text-align: center; border-radius: 15px; padding: 15px; border: 2px solid #ffffff; box-shadow: 0 4px 10px rgba(0, 0, 0, 0.2); -webkit-background-clip: text; -webkit-text-fill-color: transparent;\">Step 2: RNN Forward Pass.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8e10aa",
   "metadata": {
    "id": "m_a1AYsZJcHs",
    "papermill": {
     "duration": 0.003007,
     "end_time": "2025-12-02T05:26:35.680574",
     "exception": false,
     "start_time": "2025-12-02T05:26:35.677567",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### **Forward pass:**\n",
    "<div style=\"background:#ffffff; padding:18px; border-radius:6px; box-shadow:0 1px 2px rgba(0,0,0,0.05);\">\n",
    "<pre style=\"font-family: 'Menlo', 'Courier New', monospace; font-size:14px; line-height:1.3; margin:0;\">\n",
    "x_t --->[Wx]--\\\n",
    "                \\\n",
    "                 +--> z_t = x_t @ Wx + h_{t-1} @ Wh + b --> h_t = tanh(z_t)\n",
    "h_{t-1}-->[Wh]--/\n",
    "</pre>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6cdea97",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T05:26:35.688228Z",
     "iopub.status.busy": "2025-12-02T05:26:35.687782Z",
     "iopub.status.idle": "2025-12-02T05:26:35.696919Z",
     "shell.execute_reply": "2025-12-02T05:26:35.695830Z"
    },
    "id": "7FMQMvMh1Mjy",
    "papermill": {
     "duration": 0.014926,
     "end_time": "2025-12-02T05:26:35.698486",
     "exception": false,
     "start_time": "2025-12-02T05:26:35.683560",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rnn_step_forward(x, Wx, prev_h, Wh, b):\n",
    "  \"\"\"\n",
    "    Run the forward pass for a single timestep of a vanilla RNN that uses a tanh\n",
    "    activation function.\n",
    "\n",
    "    The input data has dimension D, the hidden state has dimension H, and we use\n",
    "    a minibatch size of N.\n",
    "\n",
    "    Args:\n",
    "        x: Input data for this timestep, of shape (N, D).\n",
    "        prev_h: Hidden state from previous timestep, of shape (N, H)\n",
    "        Wx: Weight matrix for input-to-hidden connections, of shape (D, H)\n",
    "        Wh: Weight matrix for hidden-to-hidden connections, of shape (H, H)\n",
    "        b: Biases, of shape (H,)\n",
    "\n",
    "    Returns a tuple of:\n",
    "        next_h: Next hidden state, of shape (N, H)\n",
    "        cache: Tuple of values needed for the backward pass.\n",
    "  \"\"\"\n",
    "  next_h , cache = None, None\n",
    "\n",
    "  # h t=tanh(Wx.xt + bx + Wh.ht‚àí1 + bh)\n",
    "  out = x @ Wx + prev_h @ Wh + b\n",
    "  next_h = torch.tanh(out)\n",
    "\n",
    "  # store everything needed for backward\n",
    "  cache = (x, prev_h, Wx, Wh, b, next_h)\n",
    "\n",
    "  return next_h, cache\n",
    "\n",
    "\n",
    "def rnn_forward(x, Wx, h0, Wh, b):\n",
    "  \"\"\"\n",
    "    Run a vanilla RNN forward on an entire sequence of data. We assume an input\n",
    "    sequence composed of T vectors, each of dimension D. The RNN uses a hidden\n",
    "    size of H, and we work over a minibatch containing N sequences. After running\n",
    "    the RNN forward, we return the hidden states for all timesteps.\n",
    "\n",
    "    Args:\n",
    "      x: Input data for the entire timeseries, of shape (N, T, D).\n",
    "      h0: Initial hidden state, of shape (N, H)\n",
    "      Wx: Weight matrix for input-to-hidden connections, of shape (D, H)\n",
    "      Wh: Weight matrix for hidden-to-hidden connections, of shape (H, H)\n",
    "      b: Biases, of shape (H,)\n",
    "\n",
    "    Returns a tuple of:\n",
    "        h: Hidden states for the entire timeseries, of shape (N, T, H).\n",
    "        cache: Values needed in the backward pass\n",
    "    \"\"\"\n",
    "  h, cache = None, None\n",
    "\n",
    "  N, T, D = x.shape\n",
    "  H = h0.shape[1]\n",
    "\n",
    "  cache, prev_h = [], h0\n",
    "  h = torch.zeros((N, T, H), dtype=x.dtype, device=x.device)\n",
    "\n",
    "  for t in range(T):\n",
    "    xt = x[:,t,:]\n",
    "    next_h, step_cache = rnn_step_forward(xt, Wx, prev_h, Wh, b)\n",
    "    prev_h = next_h\n",
    "    h[:,t,:] = next_h\n",
    "    cache.append(step_cache)\n",
    "\n",
    "  return h, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9007348b",
   "metadata": {
    "papermill": {
     "duration": 0.003047,
     "end_time": "2025-12-02T05:26:35.705008",
     "exception": false,
     "start_time": "2025-12-02T05:26:35.701961",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <a id=\"Import\"></a><div style=\"background: linear-gradient(to right, #1b5e20, #2e7d32, #388e3c, #43a047, #4caf50); font-family: 'Times New Roman', serif; font-size: 28px; font-weight: bold; text-align: center; border-radius: 15px; padding: 15px; border: 2px solid #ffffff; box-shadow: 0 4px 10px rgba(0, 0, 0, 0.2); -webkit-background-clip: text; -webkit-text-fill-color: transparent;\">Step 3: Explain Forward Pass.</div>\n",
    "\n",
    "## **1- Why We Loop Over **T** not **N** in an RNN**\n",
    "\n",
    "### Short Answer  \n",
    "You loop over **T** because **RNNs process sequences over time**, not batches.\n",
    "\n",
    "- **N** = number of independent examples (batch size)  \n",
    "- **T** = number of time steps in a sequence  \n",
    "\n",
    "The RNN moves step-by-step **along the time dimension** ‚Üí so you must loop over **T**, not N.\n",
    "\n",
    "---\n",
    "\n",
    "## **2- Deep Explanation (Clear and Simple)**\n",
    "\n",
    "`x` has shape **(N, T, D)**\n",
    "\n",
    "Example:\n",
    "- **Batch dimension (N)** = 2 ‚Üí processes two sequences in parallel  \n",
    "- **Time dimension (T)**  = 3 ‚Üí iterates over time steps  \n",
    "- **Feature dimension (D)** = 4 ‚Üí vector per token/time step\n",
    "\n",
    "---\n",
    "## **3- Temporal Recurrence in an RNN**\n",
    "The RNN computes hidden states **over time**:\n",
    "- **This is temporal recurrence**, because each hidden state depends on the previous hidden state.\n",
    "\n",
    "<div style=\"background:#ffffff; padding:18px; border-radius:6px; box-shadow:0 1px 2px rgba(0,0,0,0.05);\"><pre style=\"font-family: 'Menlo', 'Courier New', monospace; font-size:14px; line-height:1.3; margin:0;\">\n",
    "\n",
    "```python\n",
    "t = 0 ‚Üí compute h0 ‚Üí h1\n",
    "t = 1 ‚Üí compute h1 ‚Üí h2\n",
    "t = 2 ‚Üí compute h2 ‚Üí h3\n",
    "```\n",
    "</pre>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95898346",
   "metadata": {
    "papermill": {
     "duration": 0.002902,
     "end_time": "2025-12-02T05:26:35.710995",
     "exception": false,
     "start_time": "2025-12-02T05:26:35.708093",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <a id=\"Import\"></a><div style=\"background: linear-gradient(to right, #1b5e20, #2e7d32, #388e3c, #43a047, #4caf50); font-family: 'Times New Roman', serif; font-size: 28px; font-weight: bold; text-align: center; border-radius: 15px; padding: 15px; border: 2px solid #ffffff; box-shadow: 0 4px 10px rgba(0, 0, 0, 0.2); -webkit-background-clip: text; -webkit-text-fill-color: transparent;\">Step 4: RNN Backward Pass.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ddf52de9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T05:26:35.718932Z",
     "iopub.status.busy": "2025-12-02T05:26:35.718618Z",
     "iopub.status.idle": "2025-12-02T05:26:35.728945Z",
     "shell.execute_reply": "2025-12-02T05:26:35.727971Z"
    },
    "id": "Gepy0NyZ6KX7",
    "papermill": {
     "duration": 0.016851,
     "end_time": "2025-12-02T05:26:35.730835",
     "exception": false,
     "start_time": "2025-12-02T05:26:35.713984",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rnn_step_backward(dnext_h, cache):\n",
    "  \"\"\"\n",
    "    Backward pass for a single timestep of a vanilla RNN.\n",
    "\n",
    "    Args:\n",
    "        dnext_h: Gradient of loss with respect to next hidden state, of shape (N, H)\n",
    "        cache: Cache object from the forward pass\n",
    "\n",
    "    Returns a tuple of:\n",
    "        dx: Gradients of input data, of shape (N, D)\n",
    "        dprev_h: Gradients of previous hidden state, of shape (N, H)\n",
    "        dWx: Gradients of input-to-hidden weights, of shape (D, H)\n",
    "        dWh: Gradients of hidden-to-hidden weights, of shape (H, H)\n",
    "        db: Gradients of bias vector, of shape (H,)\n",
    "  \"\"\"\n",
    "  dx, dprev_h, dWx, dWh, db = None, None, None, None, None\n",
    "  x, prev_h, Wx, Wh, b, next_h = cache\n",
    "\n",
    "  # z = x @ Wx + prev_h @ Wh + b Equation\n",
    "\n",
    "  # Step 1: Backprop through tanh\n",
    "  dz = dnext_h * (1 - next_h**2)\n",
    "\n",
    "  # Step 2: Gradients with respect(w.r.t) to inputs and weights\n",
    "  dx = dz @ Wx.T\n",
    "  dWx = x.T @ dz\n",
    "\n",
    "  dprev_h = dz @ Wh\n",
    "  dWh = prev_h.T @ dz\n",
    "\n",
    "  # Step 3: Gradients w.r.t bais\n",
    "  db = dz.sum(dim=0) # sum over batch dimension\n",
    "\n",
    "  return dx, dprev_h, dWx, dWh, db\n",
    "\n",
    "def rnn_backward(dh, cache):\n",
    "  \"\"\"\n",
    "    Compute the backward pass  vanilla  RNN over an entire sequence of data.\n",
    "    Args:\n",
    "      dh: Upstream gradients of all hidden states, of shape (N, T, H).\n",
    "      cache : cache list storing all caches for all timesteps from the forward pass.\n",
    "\n",
    "    NOTE: 'dh' contains the upstream gradients produced by the\n",
    "    individual loss functions at each timestep, *not* the gradients\n",
    "    being passed between timesteps (which you'll have to compute yourself\n",
    "    by calling rnn_step_backward in a loop).\n",
    "\n",
    "    Returns a tuple of:\n",
    "      dx: Gradient of inputs, of shape (N, T, D)\n",
    "      dh0: Gradient of initial hidden state, of shape (N, H)\n",
    "      dWx: Gradient of input-to-hidden weights, of shape (D, H)\n",
    "      dWh: Gradient of hidden-to-hidden weights, of shape (H, H)\n",
    "      db: Gradient of biases, of shape (H,)\n",
    "  \"\"\"\n",
    "  dx, dh0, dWx, dWh, db = None, None, None, None, None\n",
    "\n",
    "  N, T, H = dh.shape\n",
    "  x0, _, Wx, Wh, _, _ = cache[0]\n",
    "  D = cache[0][0].shape[1] # or D = x0.shape[1]\n",
    "\n",
    "  # Initialize gradients\n",
    "  dx = torch.zeros((N, T, D), dtype=x0.dtype, device=x0.device)\n",
    "  dprev_h_t = torch.zeros((N, H), dtype=Wx.dtype, device=Wx.device)\n",
    "\n",
    "  dWx = torch.zeros((D, H), dtype=Wx.dtype, device=Wx.device)\n",
    "  dWh = torch.zeros((H,H), dtype=Wh.dtype, device=Wh.device)\n",
    "  db = torch.zeros((H,), dtype=Wx.dtype, device=Wx.device)\n",
    "\n",
    "  # Backprop through time (reverse order)\n",
    "  for t in reversed(range(T)):\n",
    "\n",
    "    # Cache at time step t\n",
    "    step_cached = cache[t]\n",
    "\n",
    "    # Total gradient flowing into h_t\n",
    "    # dh_total = dh_from_next_layer + dh_from_future\n",
    "    # Combine gradients from: Loss at time t ‚Üí dh[:, t, :] , Future timestep t+1 ‚Üí dprev_h_t\n",
    "    dnext_h = dh[:,t,:] + dprev_h_t\n",
    "\n",
    "    # Step backward\n",
    "    dx_t, dprev_h_t, dWx_t, dWh_t, db_t = rnn_step_backward(dnext_h, step_cached)\n",
    "\n",
    "    # Store gradients\n",
    "    dx[:,t,:] = dx_t\n",
    "    dWx +=dWx_t\n",
    "    dWh +=dWh_t\n",
    "    db +=db_t\n",
    "\n",
    "  # Gradient of initial hidden state at t0\n",
    "  dh0 = dprev_h_t\n",
    "\n",
    "  return dx, dh0, dWx, dWh, db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d421ffbb",
   "metadata": {
    "papermill": {
     "duration": 0.00316,
     "end_time": "2025-12-02T05:26:35.737195",
     "exception": false,
     "start_time": "2025-12-02T05:26:35.734035",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <a id=\"Import\"></a><div style=\"background: linear-gradient(to right, #1b5e20, #2e7d32, #388e3c, #43a047, #4caf50); font-family: 'Times New Roman', serif; font-size: 28px; font-weight: bold; text-align: center; border-radius: 15px; padding: 15px; border: 2px solid #ffffff; box-shadow: 0 4px 10px rgba(0, 0, 0, 0.2); -webkit-background-clip: text; -webkit-text-fill-color: transparent;\">Step 5: Explain Backward Pass.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1107af9d",
   "metadata": {
    "id": "N1RsINeYLxQy",
    "papermill": {
     "duration": 0.003051,
     "end_time": "2025-12-02T05:26:35.743208",
     "exception": false,
     "start_time": "2025-12-02T05:26:35.740157",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Backward pass (derivatives):**\n",
    "<div style=\"background:#ffffff; padding:18px; border-radius:6px; box-shadow:0 1px 2px rgba(0,0,0,0.05);\">\n",
    "<pre style=\"font-family: 'Menlo', 'Courier New', monospace; font-size:14px; line-height:1.3; margin:0;\">\n",
    "   dh_t\n",
    "    |\n",
    "    v\n",
    " dz = dh_t * (1 - h_t^2)\n",
    "    |\n",
    "  +---+\n",
    "  |   |\n",
    "  v   v\n",
    "dx = dz @ Wx^T      dh_prev = dz @ Wh^T\n",
    "    |\n",
    "    v\n",
    "dWx = x_t^T @ dz    dWh = h_{t-1}^T @ dz\n",
    "    |\n",
    "    v\n",
    "   db = sum(dz)\n",
    "\n",
    "</pre>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "### **Explanation:**\n",
    "- **Forward**: Compute z_t = x_t Wx + h_{t-1} Wh + b ‚Üí h_t = tanh(z_t)  \n",
    "- **Backward**: Start with upstream gradient dh_t  \n",
    "- **Through tanh**: dz = dh_t * (1 - h_t^2) (element-wise)  \n",
    "- **Linear layer**: Compute gradients w.r.t inputs and weights:\n",
    "   - dx_t = dz @ Wx^T  \n",
    "   - dh_{t-1} = dz @ Wh^T  \n",
    "   - dWx = x_t^T @ dz  \n",
    "   - dWh = h_{t-1}^T @ dz  \n",
    "   - db = dz.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7864fa0",
   "metadata": {
    "papermill": {
     "duration": 0.003013,
     "end_time": "2025-12-02T05:26:35.749562",
     "exception": false,
     "start_time": "2025-12-02T05:26:35.746549",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <a id=\"Import\"></a><div style=\"background: linear-gradient(to right, #1b5e20, #2e7d32, #388e3c, #43a047, #4caf50); font-family: 'Times New Roman', serif; font-size: 28px; font-weight: bold; text-align: center; border-radius: 15px; padding: 15px; border: 2px solid #ffffff; box-shadow: 0 4px 10px rgba(0, 0, 0, 0.2); -webkit-background-clip: text; -webkit-text-fill-color: transparent;\">Step 6: Evaluate The Code.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca8d7266",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-02T05:26:35.757724Z",
     "iopub.status.busy": "2025-12-02T05:26:35.756939Z",
     "iopub.status.idle": "2025-12-02T05:26:35.941190Z",
     "shell.execute_reply": "2025-12-02T05:26:35.939898Z"
    },
    "papermill": {
     "duration": 0.190108,
     "end_time": "2025-12-02T05:26:35.942732",
     "exception": false,
     "start_time": "2025-12-02T05:26:35.752624",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing rnn_step_forward ===\n",
      "next_h shape: torch.Size([2, 5])\n",
      "next_h Values: tensor([[-1.0000, -0.9995, -0.5591, -0.7388,  0.9966],\n",
      "        [ 0.9997, -0.9578, -0.6461,  0.5296, -0.9957]])\n",
      "\n",
      "=== Testing rnn_forward ===\n",
      "h shape: torch.Size([2, 3, 5])\n",
      "h Values: tensor([[[-1.0000, -0.9995, -0.5591, -0.7388,  0.9966],\n",
      "         [ 0.8206,  0.9999, -0.7205, -0.9238,  0.9926],\n",
      "         [ 0.9559,  0.5068, -0.9920,  0.9981, -0.7791]],\n",
      "\n",
      "        [[ 0.9997, -0.9578, -0.6461,  0.5296, -0.9957],\n",
      "         [ 1.0000, -0.7078, -0.4450,  1.0000, -1.0000],\n",
      "         [ 0.6779,  0.6420,  0.9915, -0.9779, -0.9824]]])\n",
      "\n",
      "=== Testing rnn_step_backward ===\n",
      "dx shape: torch.Size([2, 4])\n",
      "dprev_h shape: torch.Size([2, 5])\n",
      "dWx shape: torch.Size([4, 5])\n",
      "dWh shape: torch.Size([5, 5])\n",
      "db shape: torch.Size([5])\n",
      "\n",
      "=== Testing rnn_backward ===\n",
      "dx shape: torch.Size([2, 3, 4])\n",
      "dh0 shape: torch.Size([2, 5])\n",
      "dWx shape: torch.Size([4, 5])\n",
      "dWh shape: torch.Size([5, 5])\n",
      "db shape: torch.Size([5])\n",
      "\n",
      "‚úÖ All tests completed successfully!\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Dummy input and parameters\n",
    "# -----------------------------\n",
    "N, T, D, H = 2, 3, 4, 5\n",
    "torch.manual_seed(0)\n",
    "\n",
    "x = torch.randn(N, T, D)\n",
    "h0 = torch.randn(N, H)\n",
    "Wx = torch.randn(D, H)\n",
    "Wh = torch.randn(H, H)\n",
    "b = torch.randn(H)\n",
    "\n",
    "print(\"=== Testing rnn_step_forward ===\")\n",
    "x_t = x[:, 0, :]\n",
    "prev_h = h0\n",
    "next_h, step_cache = rnn_step_forward(x_t, Wx, prev_h, Wh, b)\n",
    "print(\"next_h shape:\", next_h.shape)\n",
    "print(\"next_h Values:\", next_h)\n",
    "\n",
    "print(\"\\n=== Testing rnn_forward ===\")\n",
    "h, cache = rnn_forward(x, Wx, h0, Wh, b)\n",
    "print(\"h shape:\", h.shape)\n",
    "print(\"h Values:\", h)\n",
    "\n",
    "print(\"\\n=== Testing rnn_step_backward ===\")\n",
    "dnext_h = torch.randn_like(next_h)\n",
    "dx, dprev_h, dWx, dWh, db = rnn_step_backward(dnext_h, step_cache)\n",
    "print(\"dx shape:\", dx.shape)\n",
    "print(\"dprev_h shape:\", dprev_h.shape)\n",
    "print(\"dWx shape:\", dWx.shape)\n",
    "print(\"dWh shape:\", dWh.shape)\n",
    "print(\"db shape:\", db.shape)\n",
    "\n",
    "print(\"\\n=== Testing rnn_backward ===\")\n",
    "dh = torch.randn_like(h)\n",
    "dx, dh0, dWx, dWh, db = rnn_backward(dh, cache)\n",
    "print(\"dx shape:\", dx.shape)\n",
    "print(\"dh0 shape:\", dh0.shape)\n",
    "print(\"dWx shape:\", dWx.shape)\n",
    "print(\"dWh shape:\", dWh.shape)\n",
    "print(\"db shape:\", db.shape)\n",
    "\n",
    "print(\"\\n‚úÖ All tests completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d506c5c",
   "metadata": {
    "papermill": {
     "duration": 0.003241,
     "end_time": "2025-12-02T05:26:35.949604",
     "exception": false,
     "start_time": "2025-12-02T05:26:35.946363",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <a id=\"Import\"></a><div style=\"background: linear-gradient(to right, #1b5e20, #2e7d32, #388e3c, #43a047, #4caf50); font-family: 'Times New Roman', serif; font-size: 28px; font-weight: bold; text-align: center; border-radius: 15px; padding: 15px; border: 2px solid #ffffff; box-shadow: 0 4px 10px rgba(0, 0, 0, 0.2); -webkit-background-clip: text; -webkit-text-fill-color: transparent;\">Step 7: üèÅ Conclusion.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f1002b",
   "metadata": {
    "papermill": {
     "duration": 0.003014,
     "end_time": "2025-12-02T05:26:35.955754",
     "exception": false,
     "start_time": "2025-12-02T05:26:35.952740",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In this notebook, we opened up the **black box** of Recurrent Neural Networks and built a full **vanilla RNN** from scratch ‚Äî both the **forward pass** and **backpropagation through time (BPTT)**.\n",
    "\n",
    "By implementing every step manually using only basic tensor operations, you learned:\n",
    "\n",
    "- How an RNN computes new hidden states across timesteps  \n",
    "- How gradients flow backward through time  \n",
    "- How to compute gradients w.r.t inputs, states, weights, and biases  \n",
    "- Why vanishing/exploding gradients naturally occur in RNNs  \n",
    "- How deep-learning frameworks (PyTorch, TensorFlow) compute RNN gradients internally  \n",
    "\n",
    "Understanding these internals makes you a stronger ML practitioner because you now know **what is happening behind the scenes**, not just how to call `nn.RNN`.\n",
    "\n",
    "This knowledge prepares you for learning more advanced sequence models such as **LSTM**, **GRU**, and even **Transformers**, which build on the same core ideas but add smarter gating and memory mechanisms.\n",
    "\n",
    "If you've reached this point ‚Äî congratulations üéâ  \n",
    "You now understand RNNs at a deeper level than most beginners and many practitioners.\n",
    "\n",
    "Happy learning, and keep experimenting! üöÄ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8739ce",
   "metadata": {
    "papermill": {
     "duration": 0.002898,
     "end_time": "2025-12-02T05:26:35.961678",
     "exception": false,
     "start_time": "2025-12-02T05:26:35.958780",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <a id=\"Import\"></a><div style=\"background: linear-gradient(to right, #1b5e20, #2e7d32, #388e3c, #43a047, #4caf50); font-family: 'Times New Roman', serif; font-size: 28px; font-weight: bold; text-align: center; border-radius: 15px; padding: 15px; border: 2px solid #ffffff; box-shadow: 0 4px 10px rgba(0, 0, 0, 0.2); -webkit-background-clip: text; -webkit-text-fill-color: transparent;\">Thanks & Upvote ‚ù§Ô∏è</div>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOPUAYvxLUBRMd5tYZH+t/0",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 12.143852,
   "end_time": "2025-12-02T05:26:37.388339",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-02T05:26:25.244487",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
