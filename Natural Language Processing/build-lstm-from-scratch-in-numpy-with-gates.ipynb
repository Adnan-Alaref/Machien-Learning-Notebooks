{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"authorship_tag":"ABX9TyPDvCL4BsPD3kE5Z2DCZykJ"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"background-color: black; color: white; padding: 10px;text-align: center;\">\n  <strong>Date Published:</strong> November 29, 2025 <strong>Author:</strong> Adnan Alaref\n</div>","metadata":{}},{"cell_type":"markdown","source":"# üìù Build LSTM from Scratch in NumPy (With Gates Mechanism)\n\nWelcome to this beginner-friendly notebook where we implement a **Long Short-Term Memory (LSTM)** network entirely from scratch using **NumPy**, without relying on high-level deep learning frameworks like PyTorch or TensorFlow.\n\nLSTMs are a type of **recurrent neural network (RNN)** designed to learn from **sequential data** (like time series, text, or speech). Unlike simple RNNs, LSTMs can capture **long-term dependencies** thanks to their **gate mechanism**, which controls the flow of information through the network.\n\n---\n\n## üîπ What You'll Learn\n\n- How to **build a custom LSTM class** from scratch.\n- How to handle both **single sequences** and **batch sequences**.\n- How the **input, forget, and output gates** work internally.\n- How to extract and inspect **intermediate gate values**.\n- How to test **our custom LSTM layer.**\n\n---\n\n## üîπ Why This Notebook is Beginner-Friendly\n\n- Everything is implemented using **NumPy**: no complex frameworks.\n- Step-by-step explanation of **LSTM equations**.\n- Easy-to-understand code for **forward pass** and gate computation.\n- Examples with **single sequences** and **batch processing**.\n- Gate values are accessible for **debugging and analysis**.\n\n---\n\n\n# **LSTM Equations.**\n\nFor timestep `t`, with input `x_t`, previous hidden state `h_{t-1}`, and previous cell state `c_{t-1}`:\n\n ## üîπ **Forward Pass:**\n\n$$\n\\begin{align*}\nf_t &= \\sigma(W_f x_t + U_f h_{t-1} + b_f) &\\text{Forget gate} \\\\\ni_t &= \\sigma(W_i x_t + U_i h_{t-1} + b_i) &\\text{Input gate} \\\\\n\\tilde{c}_t &= \\tanh(W_c x_t + U_c h_{t-1} + b_c) &\\text{Candidate cell state} \\\\\no_t &= \\sigma(W_o x_t + U_o h_{t-1} + b_o) &\\text{Output gate} \\\\\nc_t &= f_t \\odot c_{t-1} + i_t \\odot \\tilde{c}_t &\\text{Cell state update} \\\\\nh_t &= o_t \\odot \\tanh(c_t) &\\text{Hidden state output}\n\\end{align*}\n$$\n\nWhere:\n\n- \\(œÉ) = sigmoid activation  \n- \\(tanh) = hyperbolic tangent  \n- \\(‚äô) = element-wise multiplication  \n- \\(W_*, U_*, b_*) = weights and biases\n\n---\n\n## üîπ Backward Pass (BPTT)\n\nGradients for timestep `t`:\n\n$$\n\\begin{align*}\nd o_t &= d h_t \\odot \\tanh(c_t) \\\\\nd c_t &= d h_t \\odot o_t \\odot (1 - \\tanh^2(c_t)) + d c_{t+1} \\odot f_{t+1} \\\\\nd f_t &= d c_t \\odot c_{t-1} \\\\\nd i_t &= d c_t \\odot \\tilde{c}_t \\\\\nd \\tilde{c}_t &= d c_t \\odot i_t\n\\end{align*}\n$$\n\nPre-activation derivatives:\n\n$$\n\\begin{align*}\nd a_f &= d f_t \\odot f_t \\odot (1 - f_t) \\\\\nd a_i &= d i_t \\odot i_t \\odot (1 - i_t) \\\\\nd a_o &= d o_t \\odot o_t \\odot (1 - o_t) \\\\\nd a_c &= d \\tilde{c}_t \\odot (1 - \\tilde{c}_t^2)\n\\end{align*}\n$$\n\nGradients w.r.t weights:\n\n$$\n\\begin{align*}\ndW_* &= x_t^T \\cdot da_* \\\\\ndU_* &= h_{t-1}^T \\cdot da_* \\\\\ndb_* &= \\sum_t da_*\n\\end{align*}\n$$\n\n---\n\n## üîπ LSTM Flow Diagram\n\n```\n x_t ‚îÄ‚îÄ‚ñ∫ [W* + U* + b*] ‚îÄ‚îÄ‚ñ∫ Gates (f_t, i_t, o_t, cÃÉ_t)\n                             ‚îÇ\n                             ‚ñº\n                   Cell state c_t = f_t ‚äô c_{t-1} + i_t ‚äô cÃÉ_t\n                             ‚îÇ\n                             ‚ñº\n                    Hidden state h_t = o_t ‚äô tanh(c_t)\n\n```\n---\n\nNext, we will implement the **LSTM class in NumPy**, run **forward passes**, and inspect **gate values** step by step to see how the network processes sequential data.\n","metadata":{}},{"cell_type":"markdown","source":"# <a id=\"Import\"></a><div style=\"background: linear-gradient(to right, #1b5e20, #2e7d32, #388e3c, #43a047, #4caf50); font-family: 'Times New Roman', serif; font-size: 28px; font-weight: bold; text-align: center; border-radius: 15px; padding: 15px; border: 2px solid #ffffff; box-shadow: 0 4px 10px rgba(0, 0, 0, 0.2); -webkit-background-clip: text; -webkit-text-fill-color: transparent;\">Step 1: Import Library.</div>","metadata":{}},{"cell_type":"code","source":"import random\nimport numpy as np\n\nimport warnings\nwarnings.simplefilter(action='ignore')\nwarnings.filterwarnings(action='ignore', category=FutureWarning)","metadata":{"id":"YxJdc3RpI7sB","executionInfo":{"status":"ok","timestamp":1764390821723,"user_tz":-120,"elapsed":13,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:17:25.722203Z","iopub.execute_input":"2025-11-29T07:17:25.723251Z","iopub.status.idle":"2025-11-29T07:17:25.728868Z","shell.execute_reply.started":"2025-11-29T07:17:25.723189Z","shell.execute_reply":"2025-11-29T07:17:25.727042Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# <a id=\"Import\"></a><div style=\"background: linear-gradient(to right, #1b5e20, #2e7d32, #388e3c, #43a047, #4caf50); font-family: 'Times New Roman', serif; font-size: 28px; font-weight: bold; text-align: center; border-radius: 15px; padding: 15px; border: 2px solid #ffffff; box-shadow: 0 4px 10px rgba(0, 0, 0, 0.2); -webkit-background-clip: text; -webkit-text-fill-color: transparent;\">Step 2: Create a Custom LSTM Layer.</div>","metadata":{}},{"cell_type":"markdown","source":">\n- initial_states: Tuple of initial states (h_0, C_0), each of shape (hidden_dim,)   \n- Wf, Wi, Wc, Wo: Weight matrices for the forget, input, candidate, and output gates, respectively    \n- Uf, Ui, Uc, Uo: Recurrent weight matrices for the forget, input, candidate, and output gates, respectively    \n- bf, bi, bc, bo: Bias vectors for the forget, input, candidate, and output gates, respectively\n","metadata":{"id":"CFOu1YdJBd8F"}},{"cell_type":"code","source":"class LSTM:\n  def __init__(self, hidden_dim, input_dim) -> None:\n    self.hidden_dim = hidden_dim\n    self.input_dim = input_dim\n    self._init_params()\n\n  def _init_params(self):\n    \"\"\"Xavier-based initialization for stability\"\"\"\n    limit_U = 1 / np.sqrt(self.hidden_dim) #  For recurrent weights.\n    limit_W = np.sqrt(6 / (self.input_dim + self.hidden_dim)) #  For input weights\n\n    # Input ‚Üí Hidden weights\n    self.Wf, self.Wi, self.Wc, self.Wo = [\n      np.random.uniform(-limit_W, limit_W,(self.input_dim, self.hidden_dim))\n      for _ in range(4)\n    ]\n\n    # Hidden ‚Üí Hidden weights\n    self.Uf, self.Ui, self.Uc, self.Uo = [\n      np.random.uniform(-limit_U, limit_U,(self.hidden_dim, self.hidden_dim))\n      for _ in range(4)\n    ]\n\n    # Biases\n    self.bf, self.bi, self.bc, self.bo = [\n      np.zeros(self.hidden_dim) for _ in range(4)\n    ]\n\n\n  def _transform(self, Wx, x_t, Uh, h_t, b):\n    return np.dot(x_t, Wx) + np.dot(h_t, Uh) + b\n\n  def _gate(self, Wx, x_t, Uh, h_t, b):\n    t = self._transform(Wx, x_t, Uh, h_t, b)\n    return sigmoid(t)\n\n  def forward(self, inputs, initial_states=None, return_gates = False):\n\n    # Handle batch vs single sequence\n    if inputs.ndim == 2:\n      # Single sequence: (seq_len, input_dim)\n      inputs = inputs[:,np.newaxis,:] # Add batch dimension\n      batch_size = 1\n    else:\n      # Batch: (seq_len, batch_size, input_dim)\n      batch_size = inputs.shape[1]\n\n    seq_length = inputs.shape[0]\n    if initial_states is None:\n      h_t = np.zeros((batch_size,self.hidden_dim))\n      c_t = np.zeros((batch_size,self.hidden_dim))\n    else:\n      h_t ,c_t = initial_states\n      # Ensure states have batch dimension\n      if h_t.ndim == 1:\n        h_t = h_t[np.newaxis,:]\n        c_t = c_t[np.newaxis,:]\n\n    outputs = []\n    if return_gates:\n      gates = {\n        'f_t': [], 'i_t': [], 'o_t': [], 'fused_state': []\n      }\n\n    for t in range(seq_length):\n      x_t = inputs[t] # (batch_size, input_dim)\n\n      # Gates\n      f_t = self._gate(self.Wf, x_t, self.Uf, h_t, self.bf)\n      i_t = self._gate(self.Wi, x_t, self.Ui, h_t, self.bi)\n      o_t = self._gate(self.Wo, x_t, self.Uo, h_t, self.bo)\n\n      # Candidate state\n      fused_state = tanh(self._transform(self.Wc, x_t, self.Uc, h_t, self.bc))\n\n      # Cell update\n      c_t = f_t * c_t + i_t * fused_state\n\n      # hidden update\n      h_t = o_t * tanh(c_t)\n\n      if return_gates:\n        gates['f_t'].append(f_t.copy())\n        gates['i_t'].append(i_t.copy())\n        gates['o_t'].append(o_t.copy())\n        gates['fused_state'].append(fused_state.copy())\n      outputs.append(h_t.copy())\n\n    outputs = np.array(outputs) # (seq_len, batch_size, hidden_dim)\n    # Remove batch dimension if single sequence\n    if batch_size==1:\n      outputs = outputs[:,0,:]\n      h_t = h_t[0]\n      c_t = c_t[0]\n\n    if return_gates:\n      for k,v in gates.items():\n        gates[k] = np.array(v) # convert list ‚Üí array\n        if batch_size==1:\n          gates[k] = gates[k][:,0,:] # squeeze batch dim\n      return outputs, (h_t, c_t), gates\n    else:\n      return outputs, (h_t, c_t)\n\n  def get_parameters(self):\n    \"\"\"Return all parameters for inspection or saving\"\"\"\n    return {\n      'Wf': self.Wf, 'Wi': self.Wi, 'Wc': self.Wc, 'Wo': self.Wo,\n      'Uf': self.Uf, 'Ui': self.Ui, 'Uc': self.Uc, 'Uo': self.Uo,\n      'bf': self.bf, 'bi': self.bi, 'bc': self.bc, 'bo': self.bo\n    }\n\ndef sigmoid(x):\n  return 1 / (1 + np.exp(-np.clip(x, -50, 50)))\n\ndef tanh(x):\n  return np.tanh(np.clip(x, -50, 50))","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"txtPGano4x8d","executionInfo":{"status":"ok","timestamp":1764395870813,"user_tz":-120,"elapsed":31,"user":{"displayName":"Adnan Alaref","userId":"03256264963634723986"}},"outputId":"98927648-c149-48ba-fa24-71c16d9a99b7","trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:17:28.845676Z","iopub.execute_input":"2025-11-29T07:17:28.845938Z","iopub.status.idle":"2025-11-29T07:17:28.861508Z","shell.execute_reply.started":"2025-11-29T07:17:28.845910Z","shell.execute_reply":"2025-11-29T07:17:28.860050Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# <a id=\"Import\"></a><div style=\"background: linear-gradient(to right, #1b5e20, #2e7d32, #388e3c, #43a047, #4caf50); font-family: 'Times New Roman', serif; font-size: 28px; font-weight: bold; text-align: center; border-radius: 15px; padding: 15px; border: 2px solid #ffffff; box-shadow: 0 4px 10px rgba(0, 0, 0, 0.2); -webkit-background-clip: text; -webkit-text-fill-color: transparent;\">Step 3: Test the code.</div>","metadata":{}},{"cell_type":"code","source":"# Example usage and testing\nnp.random.seed(42)\n\n# Test 1: Single sequence\nprint(\"=== Single Sequence Test ===\")\nlstm = LSTM(hidden_dim=8, input_dim=5)\ninputs = np.random.randn(10, 5)  # (seq_len, input_dim)\n\noutputs, final_states = lstm.forward(inputs)\nprint(f\"Input shape: {inputs.shape}\")\nprint(f\"Output shape: {outputs.shape}\")\nprint(f\"Final hidden state shape: {final_states[0].shape}\")\nprint(\"\\nFinal Hidden State:\", final_states[0])\nprint(\"\\nFinal Cell State:\", final_states[1])\n\n# Test 2: Batch processing\nprint(\"\\n=== Batch Processing Test ===\")\nbatch_inputs = np.random.randn(10, 3, 5)  # (seq_len, batch_size, input_dim)\nbatch_outputs, batch_final = lstm.forward(batch_inputs)\nprint(f\"Batch input shape: {batch_inputs.shape}\")\nprint(f\"Batch output shape: {batch_outputs.shape}\")\nprint(\"\\nBatch Final Hidden State:\", batch_final[0])\nprint(\"\\nBatch Final Cell State:\", batch_final[1])\n\n# Test 3: With gate analysis\nprint(\"\\n=== Gate Analysis Test ===\")\noutputs, final_states, gates = lstm.forward(inputs, return_gates=True)\nprint(\"Gate shapes:\")\nfor gate_name, gate_values in gates.items():\n  print(f\"  {gate_name}: {gate_values.shape}\")\n\n# Test 4: Gates Parameters\nprint(\"\\n=== Gates Parameters ===\")\ngates = lstm.get_parameters()\nprint(f\"\\nWeights For Inputs gate {gates['Wi']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T07:24:28.508559Z","iopub.execute_input":"2025-11-29T07:24:28.508876Z","iopub.status.idle":"2025-11-29T07:24:28.523254Z","shell.execute_reply.started":"2025-11-29T07:24:28.508859Z","shell.execute_reply":"2025-11-29T07:24:28.522204Z"}},"outputs":[{"name":"stdout","text":"=== Single Sequence Test ===\nInput shape: (10, 5)\nOutput shape: (10, 8)\nFinal hidden state shape: (8,)\n\nFinal Hidden State: [ 0.04481557 -0.04112659 -0.12080551  0.20438796 -0.13502488 -0.15720278\n -0.09887659 -0.04118645]\n\nFinal Cell State: [ 0.20153002 -0.17672744 -0.16868528  0.34533251 -0.28118538 -0.37809598\n -0.18890819 -0.12817226]\n\n=== Batch Processing Test ===\nBatch input shape: (10, 3, 5)\nBatch output shape: (10, 3, 8)\n\nBatch Final Hidden State: [[-0.22349013 -0.04674666 -0.00363955 -0.24551477  0.07669256  0.19765127\n  -0.11742645  0.08119563]\n [-0.14642205  0.21918959  0.00480341 -0.02662936  0.55483706  0.19328816\n   0.12706896  0.19456277]\n [-0.23451168  0.31953675  0.20554991 -0.18352124 -0.08959388  0.08293861\n  -0.01321995  0.23430946]]\n\nBatch Final Cell State: [[-0.47784877 -0.1111659  -0.01011088 -0.37148196  0.13802839  0.46185492\n  -0.22905192  0.25104552]\n [-0.39467202  0.35677115  0.01108424 -0.04925037  0.99174794  0.36442915\n   0.27596584  0.46523864]\n [-0.30407864  0.47058419  0.40697907 -0.32420593 -0.10694502  0.13181451\n  -0.05933135  0.3285876 ]]\n\n=== Gate Analysis Test ===\nGate shapes:\n  f_t: (10, 8)\n  i_t: (10, 8)\n  o_t: (10, 8)\n  fused_state: (10, 8)\n\n=== Gates Parameters ===\n\nWeights For Inputs gate [[-0.51354891 -0.00655329 -0.63264142  0.55615691 -0.32775346  0.2208243\n  -0.25583427  0.02726707]\n [ 0.06346677 -0.42819847  0.63803987  0.37383189  0.59716147  0.53646473\n   0.13301988  0.57321421]\n [-0.55912859 -0.41307795 -0.61791444 -0.23732935 -0.15125778 -0.31067549\n   0.44666632 -0.1946339 ]\n [-0.29765139  0.05801255 -0.4878879   0.41060484 -0.57807184  0.66154908\n   0.3699078  -0.40936478]\n [-0.67186314  0.42862768  0.28106378  0.31115947  0.36858382 -0.57875935\n  -0.19230721 -0.52193117]]\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# <a id=\"Import\"></a><div style=\"background: linear-gradient(to right, #1b5e20, #2e7d32, #388e3c, #43a047, #4caf50); font-family: 'Times New Roman', serif; font-size: 28px; font-weight: bold; text-align: center; border-radius: 15px; padding: 15px; border: 2px solid #ffffff; box-shadow: 0 4px 10px rgba(0, 0, 0, 0.2); -webkit-background-clip: text; -webkit-text-fill-color: transparent;\">Thanks & Upvote ‚ù§Ô∏è</div>","metadata":{}}]}